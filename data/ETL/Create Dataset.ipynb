{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook is meant to turn the refined pitches into a dataset with pitch facts and the refined pitch as a ground truth"
      ],
      "metadata": {
        "id": "feid-AMa-3js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-genai pydantic tqdm"
      ],
      "metadata": {
        "id": "5-Xw0I1dQP78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzcSCiOyKCOL"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_full_pitch=\"Hi Sharks my name is pete badaway and this is my beautiful white bianca we're seeking 100 000 for 10 equity and our company sharks we want to introduce you to joe joe here he's just like all of us gets up gets himself ready puts his mask on and heads off to work the second joe leaves his house these nasty little germs which are filled with odor dirt grind and even bacteria are flying at them from all different directions [Music] i love it whether he's grabbing a quick latte yes at a crowded cafe walking past a sneezing co-worker and let's not forget all those nasties he picks up during his commute to and from work he's washing and cleaning his hands throughout the day but what's he doing about his clothes nothing sharks he does nothing that crazy nasty dirt that he's collecting throughout the day is still stuck on his clothes let's face the sharks we can't all walk around with these things on all day to keep us protected there are products to wash and clean our hands on the go but why not for our clothes which are even more exposed and that is why we created garment guard garment is the first natural garment and fabric cleanser of its kind which uses natural propellants to keep you safe and your clothes fresh and clean applying our garment spray it's easy you simply just spray it onto your clothes helps eliminate odor freshen your fabric and keep that dirt and grime under control there you go joe much better so no matter who you are or what you do garment guard gives you the freedom to live your life without having to wear one of these things on all day to keep you protected so which one of you sharks wants to team up with us to help garma guard the world?\""
      ],
      "metadata": {
        "id": "xapNx87yMoNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "item in refined_pitches_(126).json\n",
        "{\n",
        "        \"Product\": \"GarmaGuard\",\n",
        "        \"Full_Pitch\": \"Hi Sharks my name is pete badaway and this is my beautiful white bianca we're seeking 100 000 for 10 equity and our company sharks we want to introduce you to joe joe here he's just like all of us gets up gets himself ready puts his mask on and heads off to work the second joe leaves his house these nasty little germs which are filled with odor dirt grind and even bacteria are flying at them from all different directions [Music] i love it whether he's grabbing a quick latte yes at a crowded cafe walking past a sneezing co-worker and let's not forget all those nasties he picks up during his commute to and from work he's washing and cleaning his hands throughout the day but what's he doing about his clothes nothing sharks he does nothing that crazy nasty dirt that he's collecting throughout the day is still stuck on his clothes let's face the sharks we can't all walk around with these things on all day to keep us protected there are products to wash and clean our hands on the go but why not for our clothes which are even more exposed and that is why we created garment guard garment is the first natural garment and fabric cleanser of its kind which uses natural propellants to keep you safe and your clothes fresh and clean applying our garment spray it's easy you simply just spray it onto your clothes helps eliminate odor freshen your fabric and keep that dirt and grime under control there you go joe much better so no matter who you are or what you do garment guard gives you the freedom to live your life without having to wear one of these things on all day to keep you protected so which one of you sharks wants to team up with us to help garma guard the world?\"\n",
        "    }\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WvHcCK88M6vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8de379a"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "### Subtask:\n",
        "Ensure the `refined_pitches_(126).json` file is loaded into a pandas DataFrame or a list of dictionaries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "117781c4"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "### Subtask:\n",
        "Retry loading the `refined_pitches_(126).json` file into a list of dictionaries. The previous attempt failed because the file was not found. Ensure the file path is correct and the file exists before attempting to load it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6af82096",
        "outputId": "e793e89f-13f8-4aa3-a4c8-261939a18469"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "file_path = \"combined_refined_pitches_(245).json\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        pitches_data = json.load(f)\n",
        "    print(f\"Loaded {len(pitches_data)} pitch entries.\")\n",
        "else:\n",
        "    pitches_data = []\n",
        "    print(f\"File not found: {file_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 245 pitch entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8c22e35"
      },
      "source": [
        "## Initialize the gemini model\n",
        "\n",
        "### Subtask:\n",
        "Set up the Gemini 2.5 Flash model with the API key.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This was our first attempt at preparing the Dataset, we made it too easy for the models because the input was too rich"
      ],
      "metadata": {
        "id": "QH7yMftz_veI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup & imports ---\n",
        "import os, json, time, typing as t\n",
        "from pathlib import Path\n",
        "import uuid\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from pydantic import BaseModel, ValidationError\n",
        "\n",
        "# NEW: try tqdm for progress bar\n",
        "try:\n",
        "    from tqdm import tqdm  # type: ignore\n",
        "    _HAS_TQDM = True\n",
        "except Exception:\n",
        "    _HAS_TQDM = False\n",
        "\n",
        "# Optional for Colab: read key from the notebook's Secrets\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ.setdefault(\"GEMINI_API_KEY\", userdata.get(\"GEMINI_API_KEY\") or \"\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "# --- Pydantic schema (lean, aligned to your example) ---\n",
        "class InitialOffer(BaseModel):\n",
        "    amount: str\n",
        "    equity: str\n",
        "\n",
        "class ProblemStory(BaseModel):\n",
        "    persona: str\n",
        "    routine: list[str]\n",
        "    core_problem: str\n",
        "    hygiene_gap: str\n",
        "    problem_keywords: list[str]\n",
        "\n",
        "class ProductSolution(BaseModel):\n",
        "    name: str\n",
        "    product_category: str\n",
        "    key_differentiator: str\n",
        "    application: str\n",
        "    features_keywords: list[str]\n",
        "    benefits_keywords: list[str]\n",
        "\n",
        "class ClosingTheme(BaseModel):\n",
        "    call_to_action: str\n",
        "    mission: str\n",
        "    target_audience: str\n",
        "\n",
        "class PitchFacts(BaseModel):\n",
        "    founders: list[str]\n",
        "    company_name: str\n",
        "    initial_offer: InitialOffer\n",
        "    problem_story: ProblemStory\n",
        "    product_solution: ProductSolution\n",
        "    closing_theme: ClosingTheme\n",
        "\n",
        "class PitchData(BaseModel):\n",
        "    pitch_facts: PitchFacts\n",
        "\n",
        "\n",
        "# --- Prompt template ---\n",
        "PROMPT_TEMPLATE = \"\"\"You are given a startup pitch transcript.\n",
        "\n",
        "Return ONLY JSON matching this schema (no markdown, no commentary):\n",
        "PitchData = {{\n",
        "  \"pitch_facts\": {{\n",
        "    \"founders\": [str],\n",
        "    \"company_name\": str,\n",
        "    \"initial_offer\": {{\"amount\": str, \"equity\": str}},\n",
        "    \"problem_story\": {{\n",
        "      \"persona\": str,\n",
        "      \"routine\": [str],\n",
        "      \"core_problem\": str,\n",
        "      \"hygiene_gap\": str,\n",
        "      \"problem_keywords\": [str]\n",
        "    }},\n",
        "    \"product_solution\": {{\n",
        "      \"name\": str,\n",
        "      \"product_category\": str,\n",
        "      \"key_differentiator\": str,\n",
        "      \"application\": str,\n",
        "      \"features_keywords\": [str],\n",
        "      \"benefits_keywords\": [str]\n",
        "    }},\n",
        "    \"closing_theme\": {{\n",
        "      \"call_to_action\": str,\n",
        "      \"mission\": str,\n",
        "      \"target_audience\": str\n",
        "    }}\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Constraints:\n",
        "{company_name_constraint}\n",
        "\n",
        "Transcript:\n",
        "{pitch_text}\n",
        "\"\"\"\n",
        "\n",
        "def _build_prompt(pitch_text: str, exact_company_name: t.Optional[str]) -> str:\n",
        "    if exact_company_name and exact_company_name.strip():\n",
        "        constraint = (\n",
        "            f'- Use EXACTLY this string for \"company_name\": \"{exact_company_name}\". '\n",
        "            f'Do not change casing, spacing, or punctuation.'\n",
        "        )\n",
        "    else:\n",
        "        constraint = \"- If company_name is unclear, infer from the transcript.\"\n",
        "    return PROMPT_TEMPLATE.format(\n",
        "        pitch_text=pitch_text,\n",
        "        company_name_constraint=constraint,\n",
        "    )\n",
        "\n",
        "\n",
        "def _safe_write_jsonl(path: t.Union[str, Path], obj: dict) -> None:\n",
        "    path = Path(path)\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "def _call_model_with_retries(\n",
        "    client: genai.Client,\n",
        "    model: str,\n",
        "    contents: str,\n",
        "    schema_type: t.Type[BaseModel],\n",
        "    max_retries: int = 5,\n",
        "    base_delay: float = 1.0,\n",
        "    thinking_budget: int = 0,\n",
        ") -> str:\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            resp = client.models.generate_content(\n",
        "                model=model,\n",
        "                contents=contents,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    response_mime_type=\"application/json\",\n",
        "                    response_schema=schema_type,\n",
        "                    thinking_config=types.ThinkingConfig(thinking_budget=thinking_budget),\n",
        "                ),\n",
        "            )\n",
        "            return resp.text\n",
        "        except Exception as e:\n",
        "            attempt += 1\n",
        "            if attempt > max_retries:\n",
        "                raise\n",
        "            time.sleep(base_delay * (2 ** (attempt - 1)))\n",
        "\n",
        "\n",
        "def process_refined_pitches(\n",
        "    input_json_path: t.Union[str, Path] = \"refined_pitches.json\",\n",
        "    output_jsonl_path: t.Union[str, Path] = \"parsed_pitches.jsonl\",\n",
        "    errors_jsonl_path: t.Union[str, Path] = \"errors.jsonl\",\n",
        "    pitch_text_key: str = \"Full_Pitch\",\n",
        "    company_name_key: str = \"Product\",\n",
        "    model: str = \"gemini-2.5-flash\",\n",
        "    start_index: int = 0,\n",
        "    limit: t.Optional[int] = None,\n",
        "    thinking_budget: int = 0,\n",
        "    use_tqdm: bool = True,            # NEW: control loading bar\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Reads items and writes JSONL records of the form:\n",
        "    {\n",
        "      \"_meta_index\": idx,\n",
        "      \"input\": { ... pitch_facts ... },\n",
        "      \"output\": \"<original Full_Pitch>\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    input_path = Path(input_json_path)\n",
        "    if not input_path.exists():\n",
        "        raise FileNotFoundError(f\"Input not found: {input_path.resolve()}\")\n",
        "\n",
        "    with input_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(\"Input JSON must be a list of objects.\")\n",
        "\n",
        "    api_key = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\") or \"\"\n",
        "    if not api_key:\n",
        "        raise EnvironmentError(\"No GEMINI_API_KEY (or GOOGLE_API_KEY) found in environment.\")\n",
        "    client = genai.Client()\n",
        "\n",
        "    processed = successes = failures = 0\n",
        "    end_index = len(data) if limit is None else min(len(data), start_index + limit)\n",
        "    total = end_index - start_index\n",
        "\n",
        "    # truncate output files\n",
        "    Path(output_jsonl_path).write_text(\"\", encoding=\"utf-8\")\n",
        "    Path(errors_jsonl_path).write_text(\"\", encoding=\"utf-8\")\n",
        "\n",
        "    # choose iterator (with or without tqdm)\n",
        "    if use_tqdm and _HAS_TQDM:\n",
        "        iterator = tqdm(range(start_index, end_index), desc=\"Processing pitches\", unit=\"pitch\")\n",
        "    else:\n",
        "        iterator = range(start_index, end_index)\n",
        "\n",
        "    for idx in iterator:\n",
        "        item = data[idx]\n",
        "        processed += 1\n",
        "\n",
        "        pitch_text = item.get(pitch_text_key) or \"\"\n",
        "        if not isinstance(pitch_text, str) or not pitch_text.strip():\n",
        "            failures += 1\n",
        "            _safe_write_jsonl(errors_jsonl_path, {\n",
        "                \"index\": idx,\n",
        "                \"reason\": f\"Missing or empty '{pitch_text_key}'\",\n",
        "                \"item_keys\": list(item.keys())\n",
        "            })\n",
        "            # fallback progress print if no tqdm\n",
        "            if not (_HAS_TQDM and use_tqdm):\n",
        "                print(f\"[{processed}/{total}] pitch {idx} failed (empty pitch)\")\n",
        "            continue\n",
        "\n",
        "        # pull canonical company name if present\n",
        "        exact_company_name = item.get(company_name_key)\n",
        "        if isinstance(exact_company_name, str):\n",
        "            exact_company_name = exact_company_name.strip()\n",
        "        else:\n",
        "            exact_company_name = None\n",
        "\n",
        "        try:\n",
        "            prompt = _build_prompt(pitch_text, exact_company_name)\n",
        "            raw = _call_model_with_retries(\n",
        "                client=client,\n",
        "                model=model,\n",
        "                contents=prompt,\n",
        "                schema_type=PitchData,\n",
        "                thinking_budget=thinking_budget,\n",
        "            )\n",
        "\n",
        "            parsed: PitchData = PitchData.model_validate_json(raw)\n",
        "            as_dict = parsed.model_dump()\n",
        "\n",
        "            # Hard-enforce exact company name if provided\n",
        "            if exact_company_name:\n",
        "                as_dict[\"pitch_facts\"][\"company_name\"] = exact_company_name\n",
        "\n",
        "            record = {\n",
        "                \"id\": str(uuid.uuid4()),\n",
        "                \"_meta_index\": idx,\n",
        "                \"input\": as_dict[\"pitch_facts\"],\n",
        "                \"output\": pitch_text\n",
        "            }\n",
        "\n",
        "            _safe_write_jsonl(output_jsonl_path, record)\n",
        "            successes += 1\n",
        "\n",
        "        except (json.JSONDecodeError, ValidationError) as e:\n",
        "            failures += 1\n",
        "            _safe_write_jsonl(errors_jsonl_path, {\n",
        "                \"index\": idx,\n",
        "                \"error_type\": \"SchemaOrJSON\",\n",
        "                \"message\": str(e),\n",
        "                \"raw_response_excerpt\": (raw[:500] if isinstance(raw, str) else None),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            failures += 1\n",
        "            _safe_write_jsonl(errors_jsonl_path, {\n",
        "                \"index\": idx,\n",
        "                \"error_type\": \"Unhandled\",\n",
        "                \"message\": str(e),\n",
        "            })\n",
        "\n",
        "        # fallback progress print if tqdm not available\n",
        "        if not (_HAS_TQDM and use_tqdm):\n",
        "            print(f\"[{processed}/{total}] processed pitch {idx} (ok={successes}, fail={failures})\")\n",
        "\n",
        "    summary = {\n",
        "        \"input\": str(input_path),\n",
        "        \"processed\": processed,\n",
        "        \"successes\": successes,\n",
        "        \"failures\": failures,\n",
        "        \"output_jsonl\": str(Path(output_jsonl_path).resolve()),\n",
        "        \"errors_jsonl\": str(Path(errors_jsonl_path).resolve()),\n",
        "    }\n",
        "    print(\"Done.\\n\", json.dumps(summary, indent=2))\n",
        "    return summary"
      ],
      "metadata": {
        "id": "E3W2Qx1fYPfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example call ---\n",
        "summary = process_refined_pitches(\n",
        "    input_json_path=\"combined_refined_pitches_(245).json\",\n",
        "    output_jsonl_path=\"parsed_pitches.jsonl\",\n",
        "    errors_jsonl_path=\"errors.jsonl\",\n",
        "    pitch_text_key=\"Full_Pitch\",\n",
        "    model=\"gemini-2.5-flash\",   # stays fast & cheap; switch if you like\n",
        "    start_index=0,\n",
        "    limit=None,                  # or an int to process a subset\n",
        "    use_tqdm=True,           # shows bar\n",
        "    thinking_budget=0            # if you switch to a model that *requires* thinking, set > 0\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jd_2ecnYexE",
        "outputId": "3262d14c-99f2-45cb-fc10-0305981cb71a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pitches: 100%|██████████| 245/245 [10:35<00:00,  2.59s/pitch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            " {\n",
            "  \"input\": \"combined_refined_pitches_(245).json\",\n",
            "  \"processed\": 245,\n",
            "  \"successes\": 245,\n",
            "  \"failures\": 0,\n",
            "  \"output_jsonl\": \"/content/parsed_pitches.jsonl\",\n",
            "  \"errors_jsonl\": \"/content/errors.jsonl\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Dataset into train set & test set (80/20)"
      ],
      "metadata": {
        "id": "iz2ndgGrvANV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "input_filename = \"transformed.jsonl\"\n",
        "train_filename = \"train.jsonl\"\n",
        "test_filename = \"test.jsonl\"\n",
        "train_percentage = 0.8\n",
        "# ---------------------\n",
        "\n",
        "all_data = []\n",
        "\n",
        "# 1. Read all data from the input file\n",
        "try:\n",
        "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
        "        for line in infile:\n",
        "            # Skip any empty lines\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    # Check that it's valid JSON\n",
        "                    json.loads(line)\n",
        "                    all_data.append(line)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Skipping malformed JSON line: {line.strip()}\")\n",
        "\n",
        "    if not all_data:\n",
        "        print(f\"❌ Error: No data found in '{input_filename}'.\")\n",
        "        print(\"Please make sure the file is not empty.\")\n",
        "    else:\n",
        "        # 2. Shuffle the data\n",
        "        print(f\"Read {len(all_data)} total records. Shuffling...\")\n",
        "        random.shuffle(all_data)\n",
        "\n",
        "        # 3. Calculate the split point\n",
        "        split_index = int(len(all_data) * train_percentage)\n",
        "\n",
        "        # 4. Create the train and test lists\n",
        "        train_data = all_data[:split_index]\n",
        "        test_data = all_data[split_index:]\n",
        "\n",
        "        # 5. Write the train.jsonl file\n",
        "        with open(train_filename, 'w', encoding='utf-8') as outfile:\n",
        "            for line in train_data:\n",
        "                outfile.write(line) # line already includes a newline\n",
        "\n",
        "        print(f\"\\n✅ Wrote {len(train_data)} records to {train_filename}\")\n",
        "\n",
        "        # 6. Write the test.jsonl file\n",
        "        with open(test_filename, 'w', encoding='utf-8') as outfile:\n",
        "            for line in test_data:\n",
        "                outfile.write(line) # line already includes a newline\n",
        "\n",
        "        print(f\"✅ Wrote {len(test_data)} records to {test_filename}\")\n",
        "        print(\"\\nAll done!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: File not found: '{input_filename}'\")\n",
        "    print(\"Please make sure you have uploaded the file to your Colab session.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joj__QUGH-fs",
        "outputId": "5224c11f-d0fe-4885-c9d8-83cf7ebbe08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 245 total records. Shuffling...\n",
            "\n",
            "✅ Wrote 196 records to train.jsonl\n",
            "✅ Wrote 49 records to test.jsonl\n",
            "\n",
            "All done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Realised that we should remove the Founder's names from the input fields in the dataset for PII cleaning purposes"
      ],
      "metadata": {
        "id": "MaUmoWpR_RaN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "882e30ec"
      },
      "source": [
        "# Task\n",
        "Replace the \"founders\" list in each record of the \"train.jsonl\" file with a list of strings \"Founder 1\", \"Founder 2\", etc., based on the original number of founders, and save the modified data to a new file named \"train_modified.jsonl\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4dc2602"
      },
      "source": [
        "## Read data\n",
        "\n",
        "### Subtask:\n",
        "Read the data from the `train.jsonl` file into a list of dictionaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fa51ccd",
        "outputId": "3cd74b8a-446f-40fd-b8a7-19e411d02b31"
      },
      "source": [
        "import json\n",
        "\n",
        "train_data = []\n",
        "train_filename = \"sharktank_dataset.jsonl\"\n",
        "\n",
        "try:\n",
        "    with open(train_filename, 'r', encoding='utf-8') as infile:\n",
        "        for line in infile:\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    train_data.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Skipping malformed JSON line: {line.strip()}\")\n",
        "\n",
        "    print(f\"Read {len(train_data)} records from {train_filename}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: File not found: '{train_filename}'\")\n",
        "    print(\"Please make sure the file exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 245 records from sharktank_dataset.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a03f8e0",
        "outputId": "3c959366-04f2-4b3f-f89e-9941371ab1c0"
      },
      "source": [
        "modified_train_data = []\n",
        "\n",
        "for record in train_data:\n",
        "    if 'input' in record and 'founders' in record['input'] and isinstance(record['input']['founders'], list):\n",
        "        num_founders = len(record['input']['founders'])\n",
        "        record['input']['founders'] = [f\"Founder {i+1}\" for i in range(num_founders)]\n",
        "    modified_train_data.append(record)\n",
        "\n",
        "print(f\"Modified 'founders' for {len(modified_train_data)} records.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified 'founders' for 245 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cd84d12",
        "outputId": "4654b58b-00f4-476d-80ac-19018d0ba829"
      },
      "source": [
        "output_filename = \"main_modified.jsonl\"\n",
        "\n",
        "try:\n",
        "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
        "        for record in modified_train_data:\n",
        "            outfile.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"Successfully wrote modified data to {output_filename}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while writing the file: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully wrote modified data to main_modified.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dca0998f"
      },
      "source": [
        "# Task\n",
        "Transform the data from the \"sharktank_anon_dataset.jsonl\" file to match the \"To-Be\" schema provided in the user message, using the `gemini-2.5-flash` model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6408d30"
      },
      "source": [
        "## Load data\n",
        "\n",
        "### Subtask:\n",
        "Load the data from the `sharktank_dataset.jsonl` file into a list of dictionaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "930f50a8",
        "outputId": "041a2c1b-8196-48bb-e848-33e9ff0c44f5"
      },
      "source": [
        "import json\n",
        "\n",
        "all_data = []\n",
        "input_filename = \"sharktank_anon_dataset.jsonl\"\n",
        "\n",
        "try:\n",
        "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
        "        for line in infile:\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    all_data.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Skipping malformed JSON line: {line.strip()}\")\n",
        "\n",
        "    print(f\"Read {len(all_data)} records from {input_filename}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: File not found: '{input_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 245 records from sharktank_anon_dataset.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55cf08a3"
      },
      "source": [
        "# Task\n",
        "Process the \"sharktank_anon_dataset.jsonl\" file by extracting the \"output\" field (pitch), and using the Gemini 2.5 Flash model with thinking set to 0, extract information from the pitch to match the following JSON schema:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"company\": \"Frobert\",\n",
        "  \"founder\": [\"Founder 1\", \"Founder 1\"]\n",
        "  \"offer\": \"125000 for 20%\",\n",
        "  \"problem_summary\": \"Billions of pounds of cosmetically imperfect produce are wasted every year, hurting farm profitability and sustainability.\",\n",
        "  \"solution_summary\": \"Frobert turns these misfit fruits and vegetables into premium frozen desserts that reduce waste and create new revenue streams.\",\n",
        "}\n",
        "```\n",
        "\n",
        "Save the results to a new JSONL file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ae01171"
      },
      "source": [
        "## Define target schema\n",
        "\n",
        "### Subtask:\n",
        "Define the Pydantic schema for the desired output format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "367b1ef4"
      },
      "source": [
        "from pydantic import BaseModel\n",
        "import typing as t\n",
        "\n",
        "class SharkTankFacts(BaseModel):\n",
        "    company: str\n",
        "    founder: t.List[str]\n",
        "    offer: str\n",
        "    problem_summary: str\n",
        "    solution_summary: str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b43d1585"
      },
      "source": [
        "## Define prompt template\n",
        "\n",
        "### Subtask:\n",
        "Create a prompt template for the Gemini model that includes instructions and the target schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9108ae67"
      },
      "source": [
        "PROMPT_TEMPLATE_ANON = \"\"\"You are given a startup pitch transcript.\n",
        "\n",
        "Extract the following information from the transcript and return it as a JSON object matching the schema below.\n",
        "\n",
        "Return ONLY JSON matching this schema (no markdown, no commentary):\n",
        "{{\n",
        "  \"company\": str,\n",
        "  \"founder\": [str],\n",
        "  \"offer\": str,\n",
        "  \"problem_summary\": str,\n",
        "  \"solution_summary\": str\n",
        "}}\n",
        "\n",
        "Instructions:\n",
        "- For \"company\", extract the name of the company being pitched.\n",
        "- For \"founder\", create a list of strings representing the founders. Replace the actual names with \"Founder 1\", \"Founder 2\", etc., based on the number of founders mentioned.\n",
        "- For \"offer\", extract the initial investment amount and equity offered in the format \"Amount for Equity\" (e.g., \"100,000 for 10%\").\n",
        "- For \"problem_summary\", provide a concise summary of the problem the startup is trying to solve, based on the pitch.\n",
        "- For \"solution_summary\", provide a concise summary of the startup's product or service that addresses the problem, based on the pitch.\n",
        "\n",
        "Transcript:\n",
        "{pitch_text}\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c7c8b37"
      },
      "source": [
        "## Process data\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the loaded data, extract the pitch text from the \"output\" field, call the Gemini model with the prompt and pitch text, and parse the model's JSON response according to the target schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f598206",
        "outputId": "45c44737-acad-4a38-cba9-fb2ee4978019"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from google import genai\n",
        "from pydantic import ValidationError\n",
        "import typing as t\n",
        "from google.colab import userdata\n",
        "import uuid\n",
        "from tqdm import tqdm  # <-- progress bar\n",
        "\n",
        "processed_data = []\n",
        "api_key = (\n",
        "    userdata.get(\"GEMINI_API_KEY\")\n",
        "    or os.environ.get(\"GEMINI_API_KEY\")\n",
        "    or os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    or \"\"\n",
        ")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"❌ Error: No GEMINI_API_KEY (or GOOGLE_API_KEY) found in environment.\")\n",
        "else:\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    model_name = \"gemini-2.5-flash\"  # Using the specified model\n",
        "\n",
        "    # tqdm progress bar for all_data\n",
        "    for record in tqdm(all_data, desc=\"Processing pitches\", unit=\"pitch\"):\n",
        "        try:\n",
        "            pitch_text = record.get(\"output\", \"\")\n",
        "            if not pitch_text:\n",
        "                tqdm.write(f\"⚠️ Skipping record with no pitch text: {record.get('id', 'N/A')}\")\n",
        "                continue\n",
        "\n",
        "            prompt = PROMPT_TEMPLATE_ANON.format(pitch_text=pitch_text)\n",
        "\n",
        "            # Call the Gemini model\n",
        "            response = client.models.generate_content(\n",
        "                model=model_name,\n",
        "                contents=prompt,\n",
        "                config=genai.types.GenerateContentConfig(\n",
        "                    response_mime_type=\"application/json\",\n",
        "                    response_schema=SharkTankFacts,\n",
        "                    thinking_config=genai.types.ThinkingConfig(thinking_budget=0),\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            # Parse the model's JSON response\n",
        "            parsed_data = SharkTankFacts.model_validate_json(response.text)\n",
        "\n",
        "            # Create the new dictionary\n",
        "            processed_record = {\n",
        "                \"id\": record.get(\"id\", str(uuid.uuid4())),  # Use existing id or generate new one\n",
        "                \"input\": parsed_data.model_dump(),\n",
        "                \"output\": pitch_text\n",
        "            }\n",
        "            processed_data.append(processed_record)\n",
        "\n",
        "        except (json.JSONDecodeError, ValidationError) as e:\n",
        "            tqdm.write(f\"❌ Error processing record {record.get('id', 'N/A')}: JSON/validation error - {e}\")\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"❌ Unexpected error for record {record.get('id', 'N/A')}: {e}\")\n",
        "\n",
        "    print(f\"\\n✅ Successfully processed {len(processed_data)} records out of {len(all_data)}.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pitches: 100%|██████████| 245/245 [05:28<00:00,  1.34s/pitch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Successfully processed 245 records out of 245.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8290aea6",
        "outputId": "d5b43957-3363-4218-86bf-3bdeea51f9b7"
      },
      "source": [
        "import json\n",
        "\n",
        "all_data = []\n",
        "input_filename = \"sharktank_anon_dataset.jsonl\"\n",
        "\n",
        "try:\n",
        "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
        "        for line in infile:\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    all_data.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Skipping malformed JSON line: {line.strip()}\")\n",
        "\n",
        "    print(f\"Read {len(all_data)} records from {input_filename}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: File not found: '{input_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 245 records from sharktank_anon_dataset.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(0,len(processed_data)):\n",
        "  company_name = all_data[idx][\"input\"][\"company_name\"]\n",
        "  processed_data[idx][\"input\"][\"company\"] = company_name\n"
      ],
      "metadata": {
        "id": "guQ87b5YD6Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AzFg0aBBBlAq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe441586",
        "outputId": "3abee648-06d7-48eb-9c37-f899097047aa"
      },
      "source": [
        "output_filename = \"transformed.jsonl\"\n",
        "\n",
        "try:\n",
        "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
        "        for record in processed_data:\n",
        "            outfile.write(json.dumps(record) + '\\n')\n",
        "\n",
        "    print(f\"Successfully wrote transformed data to {output_filename}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while writing the file: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully wrote transformed data to transformed.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a9b5389"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Successfully read 245 records from the input file \"sharktank_anon_dataset.jsonl\".\n",
        "*   Skipped some records during transformation due to missing 'input' or 'pitch_facts' keys.\n",
        "*   Successfully transformed the data according to the specified structure.\n",
        "*   Successfully wrote the transformed data to the output file \"sharktank_anon_dataset_transformed.jsonl\".\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Investigate the records that were skipped to understand why they are missing 'input' or 'pitch_facts' keys and determine if they should be handled differently or excluded.\n",
        "*   Verify the content of the transformed data in \"sharktank_anon_dataset_transformed.jsonl\" to ensure the `company_name` was correctly inserted into the new `input` field for all processed records.\n"
      ]
    }
  ]
}